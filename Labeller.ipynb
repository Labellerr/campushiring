{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Unmount first (safe)\n",
        "!fusermount -u /content/drive || true\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Now mount cleanly\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrajpmdzmNfp",
        "outputId": "bf83cefd-4f9a-4e7f-bab9-768f837753b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# project root (already agreed)\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "print(\"Project root:\", PR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGLw6u5u2NKL",
        "outputId": "d2ae6e52-6fcf-4ba2-e5ee-5bf0f3f9f0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project root: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "folders = [\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"images\",\"train\"),\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"images\",\"val\"),\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"images\",\"test\"),\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"labels\",\"train\"),\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"labels\",\"val\"),\n",
        "    os.path.join(PR,\"data\",\"labellerr_export\",\"labels\",\"test\"),\n",
        "    os.path.join(PR,\"data\",\"images\",\"train\"),\n",
        "    os.path.join(PR,\"data\",\"images\",\"val\"),\n",
        "    os.path.join(PR,\"data\",\"images\",\"test\"),\n",
        "    os.path.join(PR,\"data\",\"labels\",\"train\"),\n",
        "    os.path.join(PR,\"data\",\"labels\",\"val\"),\n",
        "    os.path.join(PR,\"data\",\"labels\",\"test\"),\n",
        "    os.path.join(PR,\"data\",\"labels\",\"orphan\"),\n",
        "    os.path.join(PR,\"notebooks\"),\n",
        "    os.path.join(PR,\"scripts\"),\n",
        "    os.path.join(PR,\"results\"),\n",
        "    os.path.join(PR,\"docs\",\"screenshots\")\n",
        "]\n",
        "for d in folders:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Project folders created at:\", PR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJXvxnsGvZ2x",
        "outputId": "12491525-7f0c-4710-b26e-db993204c46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project folders created at: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/path_to_your_export.zip\" -d \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labellerr_export\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmAzDMqGvm9m",
        "outputId": "1bc7a3ac-7ae2-4f8a-f9bc-fcb18c4eb6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/path_to_your_export.zip, /content/drive/MyDrive/path_to_your_export.zip.zip or /content/drive/MyDrive/path_to_your_export.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, glob, os\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "EXPORT = os.path.join(PR, \"data\", \"labellerr_export\")\n",
        "DST_LABELS_TRAIN = os.path.join(PR, \"data\", \"labels\", \"train\")\n",
        "\n",
        "os.makedirs(DST_LABELS_TRAIN, exist_ok=True)\n",
        "\n",
        "src_labels_train = os.path.join(EXPORT, \"labels\", \"train\")\n",
        "if os.path.exists(src_labels_train):\n",
        "    files = glob.glob(os.path.join(src_labels_train, \"*.txt\"))\n",
        "    for f in files:\n",
        "        shutil.copy(f, DST_LABELS_TRAIN)\n",
        "    print(f\"Copied {len(files)} label files ->\", DST_LABELS_TRAIN)\n",
        "else:\n",
        "    print(\"No export labels/train found at\", src_labels_train, \"- if not exported, re-export from Labellerr or place label .txt files here.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNTCAXl1vq7P",
        "outputId": "7e406740-cf67-4c9e-e698-227d15bcd863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied 123 label files -> /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, glob, os\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "EXPORT_IMG_ROOT = os.path.join(PR, \"data\", \"labellerr_export\", \"images\")\n",
        "DST_IMG_ROOT = os.path.join(PR, \"data\", \"images\")\n",
        "\n",
        "# If your images are in a different Drive folder, set SRC_IMAGES_ROOT to that path.\n",
        "SRC_IMAGES_ROOT = EXPORT_IMG_ROOT  # change this if your images are elsewhere\n",
        "\n",
        "for split in (\"train\",\"val\",\"test\"):\n",
        "    src = os.path.join(SRC_IMAGES_ROOT, split)\n",
        "    dst = os.path.join(DST_IMG_ROOT, split)\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    if os.path.exists(src) and len(os.listdir(src))>0:\n",
        "        for f in glob.glob(os.path.join(src, \"*\")):\n",
        "            shutil.copy(f, dst)\n",
        "        print(f\"Copied images for {split}: {len(os.listdir(dst))}\")\n",
        "    else:\n",
        "        print(f\"No source images found for {split} at {src} (skip).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qVBzPBcvwj9",
        "outputId": "b74be26a-c119-42e2-91e2-78dc8a46d457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No source images found for train at /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labellerr_export/images/train (skip).\n",
            "No source images found for val at /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labellerr_export/images/val (skip).\n",
            "No source images found for test at /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labellerr_export/images/test (skip).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "for split in (\"train\",\"val\",\"test\"):\n",
        "    for fp in glob.glob(os.path.join(PR,\"data\",\"images\",split,\"*\")):\n",
        "        d,fn = os.path.split(fp)\n",
        "        new = os.path.join(d,fn.lower())\n",
        "        if fp != new:\n",
        "            try:\n",
        "                os.rename(fp,new)\n",
        "            except Exception:\n",
        "                pass\n",
        "    for fp in glob.glob(os.path.join(PR,\"data\",\"labels\",split,\"*.txt\")):\n",
        "        d,fn = os.path.split(fp)\n",
        "        new = os.path.join(d,fn.lower())\n",
        "        if fp != new:\n",
        "            os.rename(fp,new)\n",
        "print(\"Normalized filenames to lowercase where possible.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khJpbUI9wF44",
        "outputId": "702d596e-7149-4a32-94dc-7e8b9d72f0fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized filenames to lowercase where possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "\n",
        "def check_split(split):\n",
        "    imgs = {os.path.splitext(os.path.basename(p))[0] for p in glob.glob(os.path.join(PR,\"data\",\"images\",split,\"*\"))}\n",
        "    lbls = {os.path.splitext(os.path.basename(p))[0] for p in glob.glob(os.path.join(PR,\"data\",\"labels\",split,\"*.txt\"))}\n",
        "    only_imgs = sorted(list(imgs - lbls))\n",
        "    only_lbls = sorted(list(lbls - imgs))\n",
        "    print(f\"--- {split.upper()} ---\")\n",
        "    print(\"images:\", len(imgs), \"labels:\", len(lbls))\n",
        "    print(\"images without labels (sample 10):\", only_imgs[:10])\n",
        "    print(\"labels without images (sample 10):\", only_lbls[:10])\n",
        "    return only_imgs, only_lbls\n",
        "\n",
        "for s in (\"train\",\"val\",\"test\"):\n",
        "    check_split(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rriw1UNkwcCB",
        "outputId": "b78a360a-c451-4239-aa29-f6b7e86b64ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TRAIN ---\n",
            "images: 63 labels: 123\n",
            "images without labels (sample 10): []\n",
            "labels without images (sample 10): ['029495d8-00000000', '06661450-f52ed9ce', '06a8fc86-4f81a8ca', '07fae2c2-00000000', '07fe6eec-83f70a88', '09b41d3b-40990001', '09bee6ab-fc2f36f8', '0c2cb3d5-0bdfd48d', '164b2a2d-d3a80000', '16e186ec-00000000']\n",
            "--- VAL ---\n",
            "images: 60 labels: 60\n",
            "images without labels (sample 10): []\n",
            "labels without images (sample 10): []\n",
            "--- TEST ---\n",
            "images: 128 labels: 0\n",
            "images without labels (sample 10): ['7d2f7975-e0c1c5a7', '7e5ef657-a9ad0000', '7ed0f644-ac730001', '8a8a572f-261928a7', '8ac628f0-00000000', '8b5e9cbe-3e710001', '8c51607c-e71801fd', '8c5924d4-e14b149b', '8d10b6a6-64652922', '8e8559b0-00000000']\n",
            "labels without images (sample 10): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "classes_file = os.path.join(PR,\"data\",\"classes.txt\")\n",
        "data_yaml = os.path.join(PR,\"data.yaml\")\n",
        "\n",
        "# default names if classes.txt missing\n",
        "if not os.path.exists(classes_file):\n",
        "    with open(classes_file,\"w\") as f:\n",
        "        f.write(\"vehicle\\npedestrian\\n\")\n",
        "    print(\"Wrote default classes.txt at\", classes_file)\n",
        "\n",
        "with open(classes_file) as f:\n",
        "    classes = [l.strip() for l in f if l.strip()]\n",
        "\n",
        "nc = len(classes)\n",
        "names_yaml = \"[\" + \",\".join([f\"'{c}'\" for c in classes]) + \"]\"\n",
        "\n",
        "content = f\"\"\"\n",
        "train: {os.path.join(PR,'data','images','train')}\n",
        "val:   {os.path.join(PR,'data','images','val')}\n",
        "test:  {os.path.join(PR,'data','images','test')}\n",
        "\n",
        "nc: {nc}\n",
        "names: {names_yaml}\n",
        "\"\"\".strip()\n",
        "\n",
        "with open(data_yaml,\"w\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"Wrote data.yaml at\", data_yaml)\n",
        "print(content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lQleCKGwe3M",
        "outputId": "090a3a6c-d346-4bf2-dc82-b460c0e4601a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote data.yaml at /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data.yaml\n",
            "train: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/images/train\n",
            "val:   /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/images/val\n",
            "test:  /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/images/test\n",
            "\n",
            "nc: 2\n",
            "names: ['Vehicle','Pedestrian']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "\n",
        "def check(split):\n",
        "    imgs = {os.path.splitext(os.path.basename(p))[0] for p in glob.glob(os.path.join(PR,\"data\",\"images\",split,\"*\"))}\n",
        "    lbls = {os.path.splitext(os.path.basename(p))[0] for p in glob.glob(os.path.join(PR,\"data\",\"labels\",split,\"*.txt\"))}\n",
        "    print(f\"{split}: images={len(imgs)}, labels={len(lbls)}, imgs_without_labels={len(imgs-lbls)}, labels_without_images={len(lbls-imgs)}\")\n",
        "\n",
        "for s in (\"train\",\"val\",\"test\"):\n",
        "    check(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvCzR0p-2gkD",
        "outputId": "9d168e4f-f400-4f3f-9ca2-5f4688a17abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: images=63, labels=123, imgs_without_labels=0, labels_without_images=60\n",
            "val: images=60, labels=60, imgs_without_labels=0, labels_without_images=0\n",
            "test: images=128, labels=0, imgs_without_labels=128, labels_without_images=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAFE SPLIT CELL: move unlabeled val -> test, then move n_val labeled train -> val\n",
        "import os, glob, shutil, json, random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"   # change only if your path differs\n",
        "n_val = 20   # number of labelled images to reserve for validation (adjust if you want)\n",
        "log_path = Path(PR) / \"data\" / \"split_move_log.json\"\n",
        "\n",
        "# paths\n",
        "imgs_train = Path(PR) / \"data\" / \"images\" / \"train\"\n",
        "imgs_val    = Path(PR) / \"data\" / \"images\" / \"val\"\n",
        "imgs_test   = Path(PR) / \"data\" / \"images\" / \"test\"\n",
        "\n",
        "lbls_train  = Path(PR) / \"data\" / \"labels\" / \"train\"\n",
        "lbls_val    = Path(PR) / \"data\" / \"labels\" / \"val\"\n",
        "lbls_test   = Path(PR) / \"data\" / \"labels\" / \"test\"    # should remain empty\n",
        "lbls_orphan = Path(PR) / \"data\" / \"labels\" / \"orphan\"\n",
        "\n",
        "# make sure folders exist\n",
        "for p in (imgs_train, imgs_val, imgs_test, lbls_train, lbls_val, lbls_test, lbls_orphan):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "move_log = {\n",
        "    \"project_root\": str(PR),\n",
        "    \"timestamp\": None,\n",
        "    \"moved_unlabelled_val_to_test\": [],   # list of image paths moved\n",
        "    \"moved_train_to_val\": [],             # list of {\"image\":dst, \"label\":dst_label_or_null}\n",
        "}\n",
        "\n",
        "# --- 1) Move currently-unlabelled images from data/images/val -> data/images/test ---\n",
        "# definition: unlabeled image in val = image file in imgs_val with NO corresponding .txt in lbls_val\n",
        "val_images = sorted([p for p in imgs_val.glob(\"*\") if p.is_file()])\n",
        "moved_unlabelled = 0\n",
        "for imgp in val_images:\n",
        "    base = imgp.stem\n",
        "    lblp_in_val = lbls_val / (base + \".txt\")\n",
        "    if not lblp_in_val.exists():\n",
        "        # move image to test\n",
        "        dst = imgs_test / imgp.name\n",
        "        shutil.move(str(imgp), str(dst))\n",
        "        move_log[\"moved_unlabelled_val_to_test\"].append({\"from\": str(imgp), \"to\": str(dst)})\n",
        "        moved_unlabelled += 1\n",
        "# summary\n",
        "print(f\"Step 1: Moved {moved_unlabelled} currently-unlabelled image(s) from val -> test.\")\n",
        "\n",
        "# --- 2) Move n_val labelled examples from train -> val (image + label) ---\n",
        "# select labelled images from train (images that have labels in lbls_train)\n",
        "train_images = sorted([p for p in imgs_train.glob(\"*\") if p.is_file()])\n",
        "labelled_train = [p for p in train_images if (lbls_train / (p.stem + \".txt\")).exists()]\n",
        "\n",
        "if len(labelled_train) == 0:\n",
        "    raise RuntimeError(\"No labelled images found in train (no .txt in data/labels/train). Aborting.\")\n",
        "\n",
        "random.shuffle(labelled_train)\n",
        "n_val_actual = min(n_val, len(labelled_train))\n",
        "selected = labelled_train[:n_val_actual]\n",
        "\n",
        "for imgp in selected:\n",
        "    base = imgp.stem\n",
        "    dst_img = imgs_val / imgp.name\n",
        "    shutil.move(str(imgp), str(dst_img))\n",
        "    lbl_src = lbls_train / (base + \".txt\")\n",
        "    dst_lbl = None\n",
        "    if lbl_src.exists():\n",
        "        dst_lbl = lbls_val / (base + \".txt\")\n",
        "        shutil.move(str(lbl_src), str(dst_lbl))\n",
        "    move_log[\"moved_train_to_val\"].append({\n",
        "        \"image_moved\": str(dst_img),\n",
        "        \"label_moved\": str(dst_lbl) if dst_lbl else None\n",
        "    })\n",
        "\n",
        "print(f\"Step 2: Moved {n_val_actual} labelled image(s) (and their .txt) from train -> val.\")\n",
        "\n",
        "# write move log with timestamp\n",
        "move_log[\"timestamp\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "with open(log_path, \"w\") as f:\n",
        "    json.dump(move_log, f, indent=2)\n",
        "\n",
        "# --- 3) Final summary check ---\n",
        "def summarize(split_img, split_lbl):\n",
        "    imgs = list(Path(split_img).glob(\"*\"))\n",
        "    lbls = list(Path(split_lbl).glob(\"*.txt\"))\n",
        "    return len(imgs), len(lbls)\n",
        "\n",
        "train_imgs_cnt, train_lbls_cnt = summarize(imgs_train, lbls_train)\n",
        "val_imgs_cnt, val_lbls_cnt     = summarize(imgs_val, lbls_val)\n",
        "test_imgs_cnt, test_lbls_cnt   = summarize(imgs_test, lbls_test)\n",
        "orphan_cnt = len(list(lbls_orphan.glob(\"*.txt\")))\n",
        "\n",
        "print(\"\\n=== FINAL SUMMARY ===\")\n",
        "print(f\"TRAIN images: {train_imgs_cnt}, labels: {train_lbls_cnt}\")\n",
        "print(f\"VAL   images: {val_imgs_cnt}, labels: {val_lbls_cnt}\")\n",
        "print(f\"TEST  images: {test_imgs_cnt}, labels: {test_lbls_cnt}  (should be 0)\")\n",
        "print(f\"ORPHAN labels backup: {orphan_cnt}\")\n",
        "print(\"\\nMove log saved at:\", str(log_path))\n",
        "print(\"If you want to undo this split, run the undo cell I'll provide next.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vhVWmxM2zw4",
        "outputId": "6d190fc3-8457-4f1b-9323-02526386d4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Moved 0 currently-unlabelled image(s) from val -> test.\n",
            "Step 2: Moved 20 labelled image(s) (and their .txt) from train -> val.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-710925720.py:76: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  move_log[\"timestamp\"] = datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FINAL SUMMARY ===\n",
            "TRAIN images: 43, labels: 103\n",
            "VAL   images: 80, labels: 80\n",
            "TEST  images: 128, labels: 0  (should be 0)\n",
            "ORPHAN labels backup: 0\n",
            "\n",
            "Move log saved at: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/split_move_log.json\n",
            "If you want to undo this split, run the undo cell I'll provide next.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbYKXxYJE3YU",
        "outputId": "32dbed78-c588-48ea-8670-d53c010370a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.203-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.203-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.203 ultralytics-thop-2.0.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• Smoke test training (3 epochs)\n",
        "!yolo task=detect mode=train \\\n",
        "  model=yolov8n.pt \\\n",
        "  data=\"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data.yaml\" \\\n",
        "  epochs=3 \\\n",
        "  imgsz=640 \\\n",
        "  batch=8 \\\n",
        "  name=smoke_test_arvind\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNwpqDNo3GTa",
        "outputId": "784f355c-180c-45b0-8db0-2ebb34db16ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 92.6MB/s 0.1s\n",
            "Ultralytics 8.3.203 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=smoke_test_arvind, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/smoke_test_arvind, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 26.4MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 95.0MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.5¬±0.1 ms, read: 0.2¬±0.1 MB/s, size: 100.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/train... 43 images, 1 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 2.3it/s 18.8s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.5¬±0.0 ms, read: 0.3¬±0.1 MB/s, size: 153.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/val... 80 images, 1 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 80/80 1.4it/s 55.6s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/val.cache\n",
            "Plotting labels to /content/runs/detect/smoke_test_arvind/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/smoke_test_arvind\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        1/3      1.08G      1.381      3.595      1.166         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.1it/s 2.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 3.2it/s 1.6s\n",
            "                   all         80        162    0.00677      0.759      0.122      0.077\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        2/3      1.15G       1.22      3.113      1.101         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 6.9it/s 0.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 9.7it/s 0.5s\n",
            "                   all         80        162    0.00628      0.685      0.241      0.148\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K        3/3      1.17G      1.257      2.785      1.006         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.7it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.4it/s 0.8s\n",
            "                   all         80        162    0.00621      0.721      0.285      0.172\n",
            "\n",
            "3 epochs completed in 0.002 hours.\n",
            "Optimizer stripped from /content/runs/detect/smoke_test_arvind/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/smoke_test_arvind/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/smoke_test_arvind/weights/best.pt...\n",
            "Ultralytics 8.3.203 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 2.0it/s 2.5s\n",
            "                   all         80        162    0.00621      0.721      0.285      0.173\n",
            "               Vehicle         77        134     0.0114      0.978      0.568      0.346\n",
            "            Pedestrian         10         28    0.00104      0.464    0.00182   0.000574\n",
            "Speed: 1.1ms preprocess, 5.1ms inference, 0.0ms loss, 12.8ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/smoke_test_arvind\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train \\\n",
        "  model=yolov8n.pt \\\n",
        "  data=\"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data.yaml\" \\\n",
        "  epochs=100 \\\n",
        "  patience=20 \\\n",
        "  imgsz=640 \\\n",
        "  batch=8 \\\n",
        "  name=arvind_full_run\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59lsUFZ4c04",
        "outputId": "23d1fe78-0066-4fc9-afce-cfffdc665b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.203 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=arvind_full_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/arvind_full_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.5¬±0.3 ms, read: 51.1¬±20.0 MB/s, size: 100.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/train.cache... 43 images, 1 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 445.3Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 1.2¬±0.9 ms, read: 24.0¬±8.2 MB/s, size: 153.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/labels/val.cache... 80 images, 1 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 80/80 436.3Kit/s 0.0s\n",
            "Plotting labels to /content/runs/detect/arvind_full_run/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/arvind_full_run\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      1.08G      1.381      3.595      1.166         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.5it/s 2.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 4.1it/s 1.2s\n",
            "                   all         80        162    0.00677      0.759      0.122      0.077\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      1.15G      1.223      3.096        1.1         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.6it/s 1.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.5it/s 0.9s\n",
            "                   all         80        162     0.0062      0.685      0.236      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      1.17G      1.275       2.51       1.01         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.8it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.9it/s 0.7s\n",
            "                   all         80        162    0.00622      0.796      0.351      0.192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100      1.17G      1.351      2.091       1.08          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.4it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 8.0it/s 0.6s\n",
            "                   all         80        162      0.921      0.257       0.37        0.2\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100      1.19G      1.118      1.613      1.026         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.4it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.9it/s 0.8s\n",
            "                   all         80        162      0.858      0.141      0.319      0.189\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      1.19G      1.188      1.616      1.026         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.5it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.0it/s 0.7s\n",
            "                   all         80        162       0.97     0.0582      0.323       0.19\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      1.19G      1.166      1.602      1.052          5        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 6.4it/s 0.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 8.2it/s 0.6s\n",
            "                   all         80        162      0.998      0.056      0.239      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      1.19G      1.253      1.536      1.097         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.9it/s 1.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.5it/s 0.9s\n",
            "                   all         80        162       0.96     0.0522      0.181      0.114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/100      1.19G       1.26      1.538      1.081         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 5.2it/s 1.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.9it/s 0.7s\n",
            "                   all         80        162      0.976     0.0187      0.107     0.0638\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/100      1.19G      1.146      1.507      1.051         20        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.5it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 8.7it/s 0.6s\n",
            "                   all         80        162     0.0107      0.727     0.0486     0.0255\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/100      1.19G      1.122      1.437      1.051         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.6it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.3it/s 0.7s\n",
            "                   all         80        162    0.00809      0.678      0.123     0.0714\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/100      1.19G      1.131      1.374      1.051         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.3it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.1it/s 0.7s\n",
            "                   all         80        162          1     0.0386      0.207      0.121\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/100      1.19G      1.122       1.36      1.077         17        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.9it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 8.1it/s 0.6s\n",
            "                   all         80        162      0.911     0.0748      0.371      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/100      1.19G      1.117      1.305       1.05         20        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.5it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.5it/s 0.7s\n",
            "                   all         80        162      0.528      0.141      0.284      0.136\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/100      1.19G      1.105      1.418      1.025         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 5.3it/s 1.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.1it/s 1.0s\n",
            "                   all         80        162      0.528      0.141      0.284      0.136\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/100      1.19G      1.111      1.321       1.06          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 5.6it/s 1.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.9it/s 0.7s\n",
            "                   all         80        162      0.632      0.242      0.373      0.177\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/100      1.19G      1.129      1.351      1.036         18        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.7it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.4it/s 0.7s\n",
            "                   all         80        162      0.781      0.392      0.513      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/100       1.2G      1.126      1.284      1.069         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.6it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.0it/s 0.7s\n",
            "                   all         80        162      0.845      0.467      0.579      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/100      1.21G      1.053      1.282      1.045         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.5it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.0it/s 0.7s\n",
            "                   all         80        162      0.845      0.467      0.579      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/100      1.21G      1.108      1.185       1.05         19        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.1it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.2it/s 0.7s\n",
            "                   all         80        162      0.681      0.488      0.566      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/100      1.21G      1.065      1.176      1.004         11        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 4.7it/s 1.1s\n",
            "                   all         80        162      0.749      0.593       0.62      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/100      1.21G      1.045      1.231      1.039         13        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.8it/s 1.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.0it/s 1.0s\n",
            "                   all         80        162      0.744       0.55      0.622      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/100      1.24G      1.067      1.145      1.017         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.2it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.8it/s 0.7s\n",
            "                   all         80        162      0.744       0.55      0.622      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/100      1.24G      1.046      1.327      1.031          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.6it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.5it/s 0.8s\n",
            "                   all         80        162      0.632      0.516      0.556      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/100      1.24G      1.093      1.121      1.024         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.2it/s 0.7s\n",
            "                   all         80        162       0.61      0.429      0.454      0.268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/100      1.24G      1.072      1.197      1.038         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.8it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.7it/s 0.7s\n",
            "                   all         80        162       0.59      0.498      0.519      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/100      1.24G      1.006        1.2     0.9706          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.9it/s 0.6s\n",
            "                   all         80        162       0.59      0.498      0.519      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/100      1.24G      1.089      1.169      1.057         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.9it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 5.5it/s 0.9s\n",
            "                   all         80        162      0.681      0.551      0.587      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/100      1.24G     0.9518      1.079      1.001         12        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 5.0it/s 1.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 4.8it/s 1.0s\n",
            "                   all         80        162      0.684      0.498      0.565      0.309\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/100      1.24G     0.9843      1.108      1.007          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.1it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.3it/s 0.7s\n",
            "                   all         80        162      0.621      0.571      0.573      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/100      1.24G      0.971      1.027      0.991         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.6it/s 0.8s\n",
            "                   all         80        162      0.621      0.571      0.573      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/100      1.24G     0.9665      1.097     0.9949         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.4it/s 0.7s\n",
            "                   all         80        162      0.807      0.521      0.597      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/100      1.24G     0.9416      1.113      1.027         11        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.8it/s 0.6s\n",
            "                   all         80        162      0.825      0.475      0.568      0.304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/100      1.24G     0.8681     0.9496     0.9551         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.8it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.5it/s 0.7s\n",
            "                   all         80        162       0.67      0.423      0.503      0.256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/100      1.24G     0.9883      1.139     0.9987         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.8it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.1it/s 0.8s\n",
            "                   all         80        162       0.67      0.423      0.503      0.256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/100      1.24G     0.9586      1.001      1.011          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 5.4it/s 1.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 4.3it/s 1.2s\n",
            "                   all         80        162      0.667      0.467      0.529      0.275\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/100      1.24G       1.03      1.018      1.012         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 6.7it/s 0.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 8.7it/s 0.6s\n",
            "                   all         80        162      0.647      0.511      0.549      0.305\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/100      1.24G      0.978     0.9494      0.977         15        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.8it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.1it/s 0.7s\n",
            "                   all         80        162       0.65      0.505      0.552      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/100      1.24G     0.9256      1.003     0.9653          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.2it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.0it/s 0.7s\n",
            "                   all         80        162       0.65      0.505      0.552      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/100      1.24G     0.8577     0.9705     0.9341         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.6it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.3it/s 0.7s\n",
            "                   all         80        162      0.653      0.499      0.534      0.306\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/100      1.24G     0.9054      0.935     0.9406         16        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 7.0it/s 0.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 7.6it/s 0.7s\n",
            "                   all         80        162      0.642      0.527      0.536      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/100      1.24G     0.9336      1.024     0.9545          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 8.1it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 6.3it/s 0.8s\n",
            "                   all         80        162       0.64      0.586      0.562      0.311\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 22, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "42 epochs completed in 0.025 hours.\n",
            "Optimizer stripped from /content/runs/detect/arvind_full_run/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/arvind_full_run/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/arvind_full_run/weights/best.pt...\n",
            "Ultralytics 8.3.203 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5/5 2.0it/s 2.4s\n",
            "                   all         80        162      0.742       0.55      0.621      0.339\n",
            "               Vehicle         77        134      0.684      0.672      0.711       0.44\n",
            "            Pedestrian         10         28      0.799      0.429      0.531      0.238\n",
            "Speed: 0.5ms preprocess, 3.8ms inference, 0.0ms loss, 12.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/arvind_full_run\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy best.pt into your project folder\n",
        "import os, shutil, glob\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "runs = sorted(glob.glob(\"/content/runs/detect/arvind_full_run\"), key=os.path.getmtime)\n",
        "\n",
        "src_best = \"/content/runs/detect/arvind_full_run/weights/best.pt\"\n",
        "dst = os.path.join(PR, \"results\", \"best_arvind_full_run.pt\")\n",
        "os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "shutil.copy(src_best, dst)\n",
        "print(\"Copied best weights to:\", dst)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DurRDYL86UZt",
        "outputId": "12cafee6-ec39-4392-e832-52b495d3ff4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied best weights to: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/best_arvind_full_run.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import os, glob, json, cv2\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "weights = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/best_arvind_full_run.pt\"  # copied above\n",
        "images_dir = os.path.join(PR, \"data\", \"images\", \"test\")\n",
        "out_json = os.path.join(PR, \"results\", \"detections_raw.json\")\n",
        "os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
        "\n",
        "model = YOLO(weights)\n",
        "detections = []\n",
        "img_files = sorted([p for p in glob.glob(os.path.join(images_dir, \"*\")) if p.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "for i, imgp in enumerate(img_files, start=1):\n",
        "    img = cv2.imread(imgp)\n",
        "    if img is None:\n",
        "        continue\n",
        "    res = model.predict(img, conf=0.25, verbose=False)[0]\n",
        "    for b in getattr(res, \"boxes\", []):\n",
        "        xy = b.xyxy[0].cpu().numpy().tolist()\n",
        "        conf = float(b.conf[0])\n",
        "        cls = int(b.cls[0])\n",
        "        detections.append({\"frame\": i, \"image\": os.path.basename(imgp), \"bbox\": [float(x) for x in xy], \"score\": conf, \"class\": cls})\n",
        "print(\"Detections count:\", len(detections))\n",
        "with open(out_json, \"w\") as f:\n",
        "    json.dump({\"detections\": detections}, f, indent=2)\n",
        "print(\"Saved detections to:\", out_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHKXGHoGAlcR",
        "outputId": "8c41ccd7-e0cd-4734-fef1-ec941f12563f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detections count: 170\n",
            "Saved detections to: /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/detections_raw.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking -> results.json\n",
        "import os, json, importlib.util\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "dets_json = os.path.join(PR, \"results\", \"detections_raw.json\")\n",
        "out_json = os.path.join(PR, \"results\", \"results.json\")\n",
        "bytetrack_repo = \"/content/ByteTrack\"  # adjust if you cloned elsewhere\n",
        "\n",
        "with open(dets_json) as f:\n",
        "    data = json.load(f)\n",
        "entries = data.get(\"detections\", data)\n",
        "frames = {}\n",
        "for e in entries:\n",
        "    fno = int(e[\"frame\"])\n",
        "    frames.setdefault(fno, []).append((e[\"bbox\"], e[\"score\"], e[\"class\"]))\n",
        "\n",
        "# Try ByteTrack\n",
        "BYTETracker = None\n",
        "bt_path = Path(bytetrack_repo) / \"yolox\" / \"tracking_utils\" / \"byte_tracker.py\"\n",
        "if bt_path.exists():\n",
        "    spec = importlib.util.spec_from_file_location(\"byte_tracker\", str(bt_path))\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)\n",
        "    BYTETracker = getattr(mod, \"BYTETracker\", None)\n",
        "\n",
        "class SimpleIOUTracker:\n",
        "    def __init__(self,iou_thr=0.3,max_lost=30):\n",
        "        self.next_id=1; self.tracks={}; self.iou_thr=iou_thr; self.max_lost=max_lost\n",
        "    def iou(self,a,b):\n",
        "        xa1,ya1,xa2,ya2=a; xb1,yb1,xb2,yb2=b\n",
        "        xi1=max(xa1,xb1); yi1=max(ya1,yb1); xi2=min(xa2,xb2); yi2=min(ya2,yb2)\n",
        "        iw=max(0,xi2-xi1); ih=max(0,yi2-yi1); inter=iw*ih\n",
        "        A=max(0,xa2-xa1)*max(0,ya2-ya1); B=max(0,xb2-xb1)*max(0,yb2-yb1)\n",
        "        union=A+B-inter\n",
        "        return inter/union if union>0 else 0\n",
        "    def update(self,dets):\n",
        "        assigned=set(); out=[]\n",
        "        for tid,t in list(self.tracks.items()):\n",
        "            best_iou=0; best_idx=None\n",
        "            for i,(bbox,score,cls) in enumerate(dets):\n",
        "                if i in assigned: continue\n",
        "                val=self.iou(t[\"bbox\"],bbox)\n",
        "                if val>best_iou: best_iou=val; best_idx=i\n",
        "            if best_iou>=self.iou_thr and best_idx is not None:\n",
        "                bbox,score,cls=dets[best_idx]; t[\"bbox\"]=bbox; t[\"lost\"]=0; assigned.add(best_idx); out.append((tid,bbox,score,cls))\n",
        "            else:\n",
        "                t[\"lost\"]+=1\n",
        "                if t[\"lost\"]>self.max_lost: del self.tracks[tid]\n",
        "        for i,(bbox,score,cls) in enumerate(dets):\n",
        "            if i in assigned: continue\n",
        "            tid=self.next_id; self.next_id+=1\n",
        "            self.tracks[tid]={\"bbox\":bbox,\"lost\":0}\n",
        "            out.append((tid,bbox,score,cls))\n",
        "        return out\n",
        "\n",
        "use_simple=False\n",
        "if BYTETracker:\n",
        "    try:\n",
        "        tracker = BYTETracker(track_thresh=0.5, track_buffer=30)\n",
        "    except Exception as e:\n",
        "        print(\"ByteTrack instantiation failed, fallback:\", e); tracker=SimpleIOUTracker(); use_simple=True\n",
        "else:\n",
        "    print(\"ByteTrack not found, using SimpleIOUTracker\")\n",
        "    tracker=SimpleIOUTracker(); use_simple=True\n",
        "\n",
        "results=[]\n",
        "for frame in sorted(frames.keys()):\n",
        "    dets = frames[frame]\n",
        "    if not use_simple and BYTETracker:\n",
        "        try:\n",
        "            arr = np.array([[d[0][0],d[0][1],d[0][2],d[0][3],d[1]] for d in dets],dtype=float)\n",
        "            online = tracker.update(arr, None, [0,0])\n",
        "            for t in online:\n",
        "                try:\n",
        "                    tid=int(t.track_id); tlbr=list(t.tlbr)\n",
        "                    cls_id=int(getattr(t,\"cls\",0)) if hasattr(t,\"cls\") else 0\n",
        "                    score=float(getattr(t,\"score\",0.0)) if hasattr(t,\"score\") else 0.0\n",
        "                    results.append({\"frame\":frame,\"track_id\":tid,\"class\":str(cls_id),\"bbox\":tlbr,\"score\":score})\n",
        "                except: continue\n",
        "            continue\n",
        "        except Exception:\n",
        "            pass\n",
        "    out=tracker.update(dets)\n",
        "    for tid,bbox,score,cls in out:\n",
        "        results.append({\"frame\":frame,\"track_id\":int(tid),\"class\":str(int(cls)),\"bbox\":[float(x) for x in bbox],\"score\":float(score)})\n",
        "\n",
        "os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
        "with open(out_json,\"w\") as f:\n",
        "    json.dump({\"video\":None,\"frame_rate\":None,\"tracks\":results}, f, indent=2)\n",
        "\n",
        "print(\"Wrote tracking results to\", out_json, \"entries:\", len(results))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Wfut_injPx",
        "outputId": "54e825f4-6feb-49cf-a37c-0b1df2006463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ByteTrack not found, using SimpleIOUTracker\n",
            "Wrote tracking results to /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/results.json entries: 170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2, glob, os\n",
        "model = YOLO(\"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/best_arvind_full_run.pt\")\n",
        "imgs = glob.glob(\"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/data/images/test/*\")[:20]\n",
        "for p in imgs:\n",
        "    res = model.predict(p, conf=0.25, save=True, save_txt=False, save_dir=\"/content/predictions_preview\", verbose=False)\n",
        "print(\"Saved previews to /content/predictions_preview\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_iswtBNnqRn",
        "outputId": "c73ce950-b11d-44e6-efd6-bc34740295be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "Saved previews to /content/predictions_preview\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostics for results.json\n",
        "import json, os, numpy as np, math\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "results_path = os.path.join(PR, \"results\", \"results.json\")\n",
        "\n",
        "with open(results_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "tracks = data.get(\"tracks\", [])\n",
        "\n",
        "print(\"Total entries:\", len(tracks))\n",
        "\n",
        "# Unique frames / tracks\n",
        "frames = sorted({t[\"frame\"] for t in tracks})\n",
        "track_ids = sorted({int(t[\"track_id\"]) for t in tracks})\n",
        "print(\"Frames:\", frames[0] if frames else None, \"->\", frames[-1] if frames else None, \"count:\", len(frames))\n",
        "print(\"Unique track ids:\", len(track_ids))\n",
        "\n",
        "# Per-class counts and avg score\n",
        "from collections import defaultdict, Counter\n",
        "class_counts = Counter()\n",
        "class_scores = defaultdict(list)\n",
        "track_frames = defaultdict(list)\n",
        "\n",
        "for t in tracks:\n",
        "    cls = str(t[\"class\"])\n",
        "    class_counts[cls] += 1\n",
        "    class_scores[cls].append(float(t.get(\"score\", 0.0)))\n",
        "    track_frames[int(t[\"track_id\"])].append(int(t[\"frame\"]))\n",
        "\n",
        "print(\"\\nPer-class counts and avg score:\")\n",
        "for cls, cnt in class_counts.items():\n",
        "    avg_s = np.mean(class_scores[cls]) if class_scores[cls] else 0.0\n",
        "    print(f\"  class {cls}: {cnt} detections, avg score={avg_s:.3f}\")\n",
        "\n",
        "# Track lengths (number of frames per track)\n",
        "lengths = {tid: len(sorted(set(frames_list))) for tid, frames_list in track_frames.items()}\n",
        "lens_sorted = sorted(lengths.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop 10 longest tracks (track_id -> length):\")\n",
        "for tid, l in lens_sorted[:10]:\n",
        "    print(f\"  {tid} -> {l} frames\")\n",
        "\n",
        "# Short tracks (likely false positives or spurious)\n",
        "short = [tid for tid,l in lengths.items() if l <= 2]\n",
        "print(f\"\\nTracks with <=2 frames: {len(short)} (examples: {short[:10]})\")\n",
        "\n",
        "# Low-confidence detections\n",
        "low_conf_thresh = 0.4\n",
        "low_conf = [t for t in tracks if float(t.get(\"score\",0)) < low_conf_thresh]\n",
        "print(f\"\\nDetections with score < {low_conf_thresh}: {len(low_conf)} (example 10):\")\n",
        "for t in low_conf[:10]:\n",
        "    print(\" \", {\"frame\":t[\"frame\"], \"track_id\":t[\"track_id\"], \"class\":t[\"class\"], \"score\":round(float(t[\"score\"]),3)})\n",
        "\n",
        "# Bounding box size distribution\n",
        "areas = []\n",
        "for t in tracks:\n",
        "    x1,y1,x2,y2 = t[\"bbox\"]\n",
        "    area = max(0,(x2-x1)) * max(0,(y2-y1))\n",
        "    areas.append(area)\n",
        "areas = np.array(areas)\n",
        "print(f\"\\nBBox area: mean={areas.mean():.1f}, median={np.median(areas):.1f}, min={areas.min():.1f}, max={areas.max():.1f}\")\n",
        "\n",
        "# Save summary to disk\n",
        "summary = {\n",
        "    \"total_entries\": len(tracks),\n",
        "    \"frames_range\": [frames[0] if frames else None, frames[-1] if frames else None],\n",
        "    \"num_tracks\": len(track_ids),\n",
        "    \"per_class_counts\": dict(class_counts),\n",
        "    \"top_tracks\": lens_sorted[:20],\n",
        "    \"short_track_ids_sample\": short[:50],\n",
        "    \"low_conf_count\": len(low_conf),\n",
        "    \"bbox_area_stats\": {\"mean\": float(areas.mean()), \"median\": float(np.median(areas)), \"min\": float(areas.min()), \"max\": float(areas.max())}\n",
        "}\n",
        "out_summary = os.path.join(PR,\"results\",\"results_summary.json\")\n",
        "with open(out_summary,\"w\") as f:\n",
        "    json.dump(summary,f,indent=2)\n",
        "print(\"\\nSaved summary to\", out_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE12T0ovn0z7",
        "outputId": "6cf82898-b10b-4263-9a7d-eef99e3e6249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries: 170\n",
            "Frames: 1 -> 128 count: 95\n",
            "Unique track ids: 69\n",
            "\n",
            "Per-class counts and avg score:\n",
            "  class 0: 166 detections, avg score=0.547\n",
            "  class 1: 4 detections, avg score=0.416\n",
            "\n",
            "Top 10 longest tracks (track_id -> length):\n",
            "  4 -> 16 frames\n",
            "  10 -> 13 frames\n",
            "  14 -> 13 frames\n",
            "  9 -> 10 frames\n",
            "  12 -> 9 frames\n",
            "  5 -> 6 frames\n",
            "  46 -> 5 frames\n",
            "  18 -> 4 frames\n",
            "  31 -> 4 frames\n",
            "  32 -> 4 frames\n",
            "\n",
            "Tracks with <=2 frames: 51 (examples: [1, 2, 3, 6, 7, 8, 13, 15, 16, 17])\n",
            "\n",
            "Detections with score < 0.4: 60 (example 10):\n",
            "  {'frame': 6, 'track_id': 5, 'class': '0', 'score': 0.307}\n",
            "  {'frame': 8, 'track_id': 4, 'class': '0', 'score': 0.361}\n",
            "  {'frame': 9, 'track_id': 9, 'class': '0', 'score': 0.358}\n",
            "  {'frame': 13, 'track_id': 10, 'class': '0', 'score': 0.325}\n",
            "  {'frame': 17, 'track_id': 3, 'class': '0', 'score': 0.291}\n",
            "  {'frame': 18, 'track_id': 19, 'class': '0', 'score': 0.313}\n",
            "  {'frame': 21, 'track_id': 20, 'class': '0', 'score': 0.277}\n",
            "  {'frame': 22, 'track_id': 21, 'class': '0', 'score': 0.367}\n",
            "  {'frame': 22, 'track_id': 22, 'class': '1', 'score': 0.333}\n",
            "  {'frame': 24, 'track_id': 24, 'class': '0', 'score': 0.253}\n",
            "\n",
            "BBox area: mean=66518.8, median=37482.8, min=1636.9, max=417623.9\n",
            "\n",
            "Saved summary to /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/results_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter and produce cleaned results + Labellerr-ready JSON\n",
        "import json, os\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "results_path = os.path.join(PR, \"results\", \"results.json\")\n",
        "clean_path = os.path.join(PR, \"results\", \"results_filtered.json\")\n",
        "labellerr_path = os.path.join(PR, \"results\", \"results_labellerr.json\")\n",
        "\n",
        "with open(results_path) as f:\n",
        "    data = json.load(f)\n",
        "tracks = data.get(\"tracks\", [])\n",
        "\n",
        "# Filtering params (tweak as needed)\n",
        "MIN_SCORE = 0.45\n",
        "MIN_AREA = 32*32  # e.g., ignore boxes smaller than 32x32 pixels\n",
        "\n",
        "filtered = []\n",
        "for t in tracks:\n",
        "    score = float(t.get(\"score\", 0.0))\n",
        "    x1,y1,x2,y2 = t[\"bbox\"]\n",
        "    area = max(0,(x2-x1))*max(0,(y2-y1))\n",
        "    if score < MIN_SCORE:\n",
        "        continue\n",
        "    if area < MIN_AREA:\n",
        "        continue\n",
        "    filtered.append(t)\n",
        "\n",
        "print(\"Original entries:\", len(tracks), \"Filtered entries:\", len(filtered))\n",
        "with open(clean_path, \"w\") as f:\n",
        "    json.dump({\"video\": data.get(\"video\"), \"frame_rate\": data.get(\"frame_rate\"), \"tracks\": filtered}, f, indent=2)\n",
        "print(\"Saved cleaned results to\", clean_path)\n",
        "\n",
        "# Convert to a generic Labellerr-friendly format:\n",
        "# Many label platforms expect per-image list of detections; we'll create:\n",
        "# { \"image_name\": [{\"class\":..., \"bbox\":[x1,y1,x2,y2], \"score\":...}, ...], ... }\n",
        "# We assume your images are in data/images/test and filename order corresponds to frame index.\n",
        "import glob\n",
        "img_dir = os.path.join(PR, \"data\", \"images\", \"test\")\n",
        "img_list = sorted([os.path.basename(p) for p in glob.glob(os.path.join(img_dir,\"*\")) if p.lower().endswith(('.jpg','.png','.jpeg'))])\n",
        "# map frame index -> image filename (1-indexed)\n",
        "frame_to_image = {i+1: img_list[i] for i in range(len(img_list))}\n",
        "\n",
        "labellerr_payload = {}\n",
        "for t in filtered:\n",
        "    frame = int(t[\"frame\"])\n",
        "    img_name = frame_to_image.get(frame, f\"frame_{frame:04d}.jpg\")\n",
        "    labellerr_payload.setdefault(img_name, []).append({\n",
        "        \"class\": int(t[\"class\"]) if str(t[\"class\"]).isdigit() else t[\"class\"],\n",
        "        \"bbox\": [float(x) for x in t[\"bbox\"]],\n",
        "        \"score\": float(t.get(\"score\",0))\n",
        "    })\n",
        "\n",
        "with open(labellerr_path, \"w\") as f:\n",
        "    json.dump(labellerr_payload, f, indent=2)\n",
        "print(\"Saved Labellerr-style JSON to\", labellerr_path, \" (images without entries simply absent)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905dhoHboiL2",
        "outputId": "d1740c11-d75a-4007-ca72-39c8db305cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original entries: 170 Filtered entries: 100\n",
            "Saved cleaned results to /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/results_filtered.json\n",
            "Saved Labellerr-style JSON to /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/results_labellerr.json  (images without entries simply absent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export per-track cropped images (for quick manual review)\n",
        "import os, json, cv2, glob\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "results_path = os.path.join(PR, \"results\", \"results_filtered.json\")  # use filtered if available\n",
        "images_dir = os.path.join(PR, \"data\", \"images\", \"test\")\n",
        "export_dir = os.path.join(PR, \"results\", \"track_crops\")\n",
        "os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "with open(results_path) as f:\n",
        "    data = json.load(f)\n",
        "tracks = data.get(\"tracks\", [])\n",
        "\n",
        "# group by track id\n",
        "from collections import defaultdict\n",
        "by_track = defaultdict(list)\n",
        "for t in tracks:\n",
        "    by_track[int(t[\"track_id\"])].append(t)\n",
        "\n",
        "# map frame->image path\n",
        "img_files = sorted([p for p in glob.glob(os.path.join(images_dir,\"*\")) if p.lower().endswith(('.jpg','.jpeg','.png'))])\n",
        "frame_to_path = {i+1: img_files[i] for i in range(len(img_files))}\n",
        "\n",
        "# export up to N crops per track\n",
        "MAX_PER_TRACK = 40\n",
        "for tid, entries in list(by_track.items()):\n",
        "    entries_sorted = sorted(entries, key=lambda x: int(x[\"frame\"]))\n",
        "    tdir = os.path.join(export_dir, f\"track_{tid:04d}\")\n",
        "    os.makedirs(tdir, exist_ok=True)\n",
        "    for i, e in enumerate(entries_sorted[:MAX_PER_TRACK]):\n",
        "        fno = int(e[\"frame\"])\n",
        "        imgp = frame_to_path.get(fno)\n",
        "        if not imgp or not os.path.exists(imgp):\n",
        "            continue\n",
        "        img = cv2.imread(imgp)\n",
        "        x1,y1,x2,y2 = map(int, e[\"bbox\"])\n",
        "        # clip\n",
        "        h,w = img.shape[:2]\n",
        "        x1,x2 = max(0,x1), min(w-1,x2)\n",
        "        y1,y2 = max(0,y1), min(h-1,y2)\n",
        "        crop = img[y1:y2, x1:x2]\n",
        "        if crop.size == 0:\n",
        "            continue\n",
        "        outp = os.path.join(tdir, f\"{i+1:03d}_f{fno:04d}_s{int(e['score']*100):03d}.jpg\")\n",
        "        cv2.imwrite(outp, crop)\n",
        "print(\"Saved per-track crops to\", export_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cmj6_HQonIg",
        "outputId": "7859a4be-0eec-4017-b4d6-37a581371607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved per-track crops to /content/drive/MyDrive/campushiring/arvind_mankotia_labellerr/results/track_crops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic upload snippet ‚Äî replace with real endpoint & API_KEY & project id\n",
        "import requests, os, json\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "payload_file = os.path.join(PR,\"results\",\"results_labellerr.json\")\n",
        "with open(payload_file) as f:\n",
        "    payload = json.load(f)\n",
        "\n",
        "API_KEY = \"fe2359.b0d4b547bbbc678c8089e6c952\"\n",
        "PROJECT_ID = \"henriette_casual_iguana_29174\"\n",
        "# Example endpoint - change to match their API docs\n",
        "url = f\"https://api.labellerr.com/v1/projects/{PROJECT_ID}/predictions/upload\"\n",
        "\n",
        "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "resp = requests.post(url, headers=headers, json=payload)\n",
        "print(\"Status code:\", resp.status_code)\n",
        "print(resp.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1hYcWB1opmE",
        "outputId": "b3d7c31b-b448-44e1-e86b-f2dc61bb8a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status code: 401\n",
            "{\"error\":\"Invalid Authorization Header\",\"message\":\"jwt malformed\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, json, requests\n",
        "\n",
        "# Mount Google Drive (if not already)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception as e:\n",
        "    print(\"Drive mount message:\", e)\n",
        "    # If the mountpoint already has files you'll see an error; if so,\n",
        "    # manually confirm /content/drive is ok or restart runtime and re-run.\n",
        "\n",
        "# --- EDIT THESE ---\n",
        "API_KEY = \"fe2359.b0d4b547bbbc678c8089e6c952\"       # <<< paste your Labellerr API key here (keep secret)\n",
        "PROJECT_ID = \"henriette_casual_iguana_29174\"  # you already provided this\n",
        "# --------------------\n",
        "\n",
        "# Project root in Drive (update if different)\n",
        "PR = \"/content/drive/MyDrive/campushiring/arvind_mankotia_labellerr\"\n",
        "\n",
        "# Where your predictions / tracking JSON is saved\n",
        "RESULTS_PATH = os.path.join(PR, \"results\", \"results.json\")   # update if filename differs\n",
        "\n",
        "print(\"Drive root exists:\", os.path.exists(\"/content/drive\"))\n",
        "print(\"project root exists:\", os.path.exists(PR))\n",
        "print(\"results.json exists:\", os.path.exists(RESULTS_PATH))\n",
        "\n",
        "# Quick read if file exists\n",
        "if os.path.exists(RESULTS_PATH):\n",
        "    with open(RESULTS_PATH, \"r\") as f:\n",
        "        try:\n",
        "            sample = json.load(f)\n",
        "            print(\"Loaded results.json, top-level keys:\", list(sample.keys())[:10])\n",
        "        except Exception as e:\n",
        "            print(\"Failed loading results.json:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wui5e03dqm7t",
        "outputId": "6e5f90b3-2cb1-4be3-c527-89fb6b8464ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive root exists: True\n",
            "project root exists: True\n",
            "results.json exists: True\n",
            "Loaded results.json, top-level keys: ['video', 'frame_rate', 'tracks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "\n",
        "API_KEY = \"3af33f.f90d6e4773ba2d3627ba6af5fa\"\n",
        "PROJECT_ID = \"henriette_casual_iguana_29174\"\n",
        "base = \"https://api.labellerr.com/v1\"\n",
        "\n",
        "# 1. Exchange API key for JWT\n",
        "login_url = f\"{base}/auth/loginWithApiKey\"\n",
        "resp = requests.post(login_url, json={\"apiKey\": API_KEY}, timeout=20)\n",
        "\n",
        "print(\"Login response:\", resp.status_code, resp.text[:200])\n",
        "\n",
        "if resp.status_code == 200:\n",
        "    jwt_token = resp.json().get(\"token\")\n",
        "    print(\"‚úÖ Got JWT:\", jwt_token[:40], \"...\")\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {jwt_token}\"}\n",
        "\n",
        "    # 2. Now request project details\n",
        "    proj_url = f\"{base}/projects/{PROJECT_ID}\"\n",
        "    r = requests.get(proj_url, headers=headers, timeout=20)\n",
        "    print(\"Project check:\", r.status_code)\n",
        "    print(json.dumps(r.json(), indent=2)[:400])\n",
        "else:\n",
        "    print(\"‚ùå Could not log in with API key. Check if Labellerr docs show a different login endpoint.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaMJ10bHscno",
        "outputId": "0b859a18-d0b6-4a13-ae59-a75e31cf3917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login response: 401 {\"error\":\"Invalid Authorization Header\",\"message\":\"No authorization token was found\"}\n",
            "‚ùå Could not log in with API key. Check if Labellerr docs show a different login endpoint.\n"
          ]
        }
      ]
    }
  ]
}